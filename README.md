
```
â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•
Intelligent Road Hazard Analysis System
```
![Build](https://img.shields.io/badge/build-passing-brightgreen)
![Status](https://img.shields.io/badge/status-WIP-yellow)
![Performance](https://img.shields.io/badge/FPS-31--35-blue)
![YOLO](https://img.shields.io/badge/YOLO-v8n-orange)
![Transformer](https://img.shields.io/badge/Transformer-enabled-purple)
![CUDA](https://img.shields.io/badge/CUDA-12.1-informational)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Hardware](https://img.shields.io/badge/GPU-RTX_4070_Ti-red)
![License](https://img.shields.io/badge/license-private-lightgrey)

IRHAS is a modular real-time road hazard analysis system that fuses object detection and vehicle motion data to assess driving threats.

---

## âœ¨ Key Features
- ğŸ” Detection of vehicles, pedestrians, traffic lights, bicycles, and obstacles
- âš¡ *Parallel* inference of *N* YOLOv8n models*
- ğŸ§  Detection fusion powered by a transformer module
- ğŸš˜ Ego-motion integration (speed, acceleration, steering, curvature)
- ğŸš¨ Automatic alert levels: SAFE / WARNING / CRITICAL
- ğŸ¥ Real-time overlay visualization on video frames

---

## ğŸ§© IRHAS Architecture (ASCII DIAGRAM)
```
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚      VIDEO INPUT (FHD)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ PREPROCESSING   â”‚
                              â”‚ 1280Ã—720 resize â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€ P A R A L L E L   M O D E L   C L U S T E R â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                                           â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
 â”‚ YOLO: CARS  â”‚ â”‚ YOLO: PERSON    â”‚ â”‚ YOLO: TRAFFIC   â”‚ â”‚ YOLO: OBSTACLES â”‚
 â”‚ specialized â”‚ â”‚ specialized     â”‚ â”‚ LIGHTS (spec)   â”‚ â”‚/ BICYCLES (spec)â”‚
 â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼         â–¼                 â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      TRANSFORMER FUSION ENGINE        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚ fused detections
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚ EGO-MOTION DATA â”‚ speed/accel/steer
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚  THREAT SCORE   â”‚ zone Ã— class Ã— dynamics
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ ALERT ENGINE + OVERLAY    â”‚
                        â”‚ SAFE / WARNING / CRIT     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     REAL-TIME DISPLAY     â”‚
                        â”‚         31â€“35 FPS         â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Estimated Runtime & Performance
> âš ï¸ Note: Values below are **predicted** based on model size, parallel inference assumptions and hardware targets.  
> Actual performance may **very vary** depending on GPU, drivers and runtime optimization.

| Metric | Estimated Value |
|--------|-----------------|
| Input Resolution | 1920Ã—1080 (FHD) |
| Processing Resolution | 1280Ã—720 (HD) |
| Latency | ~28â€“32 ms / frame |
| Output FPS | ~31â€“35 FPS |
| VRAM Usage | ~7â€“8 GB |
| CPU Load | ~30â€“40% |
| GPU Load | ~70â€“80% |

---

## ğŸ¯ Why IRHAS?
- ğŸ§© Lightweight modular architecture: instead of one heavy multi-class model, IRHAS uses several small specialized models.
- âš¡ On-demand activation: we load and run only the model(s) required for the current scene - minimizing concurrent memory and compute usage.
- ğŸª¶ Low resource footprint: fewer simultaneous instances => lower VRAM/CPU load and reduced latency.
- ğŸ“ˆ Higher effectiveness at lower cost: better mAP with less runtime overhead compared to monolithic detectors.
- ğŸš— Edge-ready: optimized for consumer and embedded hardware via quantization and selective inference.

---

## ğŸ§ª Dataset & Training
- ğŸ“š Source: PhysicalAI (HuggingFace)  
- ğŸ¤– Auto-label workflow: Autodistill + GroundingDINO/SAM  
- ğŸ§± N independent YOLOv8n models trained on specialized classes*  
- ğŸ§® INT8 model quantization  
- ğŸ”€ Train/val/test split: 70 / 15 / 15

---

## ğŸ—ºï¸ Roadmap
- [x] Dataset collection + preprocessing
- [ ] Auto annotation + validation
- [ ] YOLOv8 specialist model training
- [ ] Parallel inference implementation
- [ ] Transformer fusion + threat scoring
- [ ] CUDA/TensorRT optimization
- [ ] Edge/Jetson deployment
- [ ] Demo video, telemetry + benchmarking

---

## ğŸ“Œ Project Status
âš ï¸ Active development



<br>
<small><sup>*</sup> Default configuration includes 4 specialist models (e.g. cars, pedestrians, traffic lights, obstacles). IRHAS supports configuring <b><i>N</i></b> models so users can add or remove specialist detectors. Models can be deployed as a fixed set (e.g., 4) or <i><b>hot-swapped</b></i> at runtime: only currently active models are loaded/executed to minimize VRAM and CPU usage. This enables flexible trade-offs between detection coverage and resource footprint; recommended default = 4 for typical edge/desktop deployments.</small>

